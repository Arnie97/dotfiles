#!/usr/bin/env python3

import hashlib
import os
import sys
from operator import itemgetter


def md5sum(filename, chunk_size=8192):
    md5 = hashlib.md5()
    with open(filename, 'rb') as f:
        for chunk in iter(lambda: f.read(chunk_size), b''):
            md5.update(chunk)
    return md5.hexdigest()


def main(checksum_file, *directories):
    # try to create a backup, but refuse to overwrite an existing backup
    checksum_file = os.path.abspath(checksum_file)
    checksum_bak = checksum_file + '.bak'
    if os.path.exists(checksum_bak):
        raise FileExistsError(checksum_bak)

    # list all files in the specified directories and their subdirectories
    checksum_root = os.path.dirname(checksum_file)
    if not directories:
        directories = [checksum_root]
    else:
        directories = [os.path.abspath(root) for root in directories]
    os.chdir(checksum_root)
    new_files = {
        os.path.relpath(os.path.join(path, file))
        for root in directories
        for path, dirs, files in os.walk(root)
        for file in files
        if not file.endswith('.md5')
    }

    # ignore file entries that exist on disk
    checksums = {}
    with open(checksum_file, encoding='utf-8') as src:
        for line_no, raw_line in enumerate(src):
            line = raw_line.strip()
            if len(line) == 0 or line[1] in '#;':
                continue
            checksum, _, file = line.partition('  ')
            if file in new_files:
                new_files.discard(file)
                continue
            checksums[checksum] = line_no, file

    # try to match moved or renamed entries by MD5 checksums
    calculated_files = len(new_files)
    new_files = {md5sum(file): file for file in new_files}
    unique_files = len(new_files)
    modified_lines = {
        line_no: (checksum, old_file, new_files.pop(checksum, None))
        for checksum, (line_no, old_file) in checksums.items()
    }

    os.rename(checksum_file, checksum_bak)
    with open(checksum_file, 'x', encoding='utf-8') as dest:
        # update the paths of moved files
        with open(checksum_bak, encoding='utf-8') as src:
            # write existing entries in the original order
            line_no = 0
            for line_no, raw_line in enumerate(src, 1):
                if line_no not in modified_lines:
                    dest.write(raw_line)
                    continue

                checksum, old_file, new_file = modified_lines[line_no]
                if new_file:
                    dest.write('{}  {}\n'.format(checksum, new_file))
                    sys.stdout.write('~ {} {} -> {}\n'.format(
                        line_no, old_file, new_file))
                else:
                    dest.write(raw_line)
                    sys.stdout.write('? {} {}\n'.format(line_no, old_file))

        # then append new entries at the end
        new_files = sorted(new_files.items(), key=itemgetter(1))
        for line_no, (checksum, new_file) in enumerate(new_files, line_no + 1):
            dest.write('{}  {}\n'.format(checksum, new_file))
            sys.stdout.write('+ {} {}\n'.format(line_no, new_file))

    summary = '  '.join('{}: {}'.format(k, v) for k, v in [
        ('Total', line_no),
        ('Success', line_no - len(modified_lines) - len(new_files)),
        ('Moved', unique_files - len(new_files)),
        ('Miss', len(modified_lines) - (unique_files - len(new_files))),
        ('Duplicate', calculated_files - unique_files),
        ('New', len(new_files)),
    ])
    sys.stdout.write('-' * 80 + '\n' + summary + '\n')


if __name__ == '__main__':
    if len(sys.argv) < 2:
        sys.stderr.write('usage: hsync <file> [directory...]\n')
    else:
        main(*sys.argv[1:])
